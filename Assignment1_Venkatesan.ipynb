{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe5be4f",
   "metadata": {},
   "source": [
    "# Assignment 1: Working with Terms and Documents\n",
    "\n",
    "This first homework assignment starts off with term statistics computations and graphing. In the final section (for CS6200 students), you collect new documents to experiment with.\n",
    "\n",
    "Read through this Jupyter notebook and fill in the parts marked with `TODO`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b9318",
   "metadata": {},
   "source": [
    "## Download and Unzip the Sample Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698c78d",
   "metadata": {},
   "source": [
    "Your first task is to download a dataset containing counts of terms in documents from \"https://github.com/dasmiq/cs6200-hw1/blob/main/ap201001.json.gz?raw=true\". The dataset covers the first one million tokens in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download a zipped JSON file from Github. (1 points)\n",
    "# Hints: use \"!wget\"\n",
    "#need to do this\n",
    "\n",
    "!wsl wget \"https://github.com/dasmiq/cs6200-hw1/blob/main/ap201001.json.gz?raw=true\" -O ap201001.json.gz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Unzip the file to access the JSON data. Make sure you have a file named ap201001.json after unzipping. (1 points)\n",
    "# Hints: use \"!gunzip\"\n",
    "\n",
    "!wsl gunzip ap201001.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# TODO: Convert ap201001.json file with one JSON record on each line to a list of dictionaries.(1 points)\n",
    "\n",
    "rawfile= open('ap201001.json')\n",
    "terms = [json.loads(line) for line in rawfile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c91da",
   "metadata": {},
   "source": [
    "Now that you've successfully downloaded and unzipped the data, let's dig deeper. Your task is to explore some basic statistics about the terms in our dataset. This will give you a better understanding of the data you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b25026",
   "metadata": {},
   "source": [
    "Find the first 10 terms from the document. In this dataset, field only takes the values `body` or `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find first 10 terms. (2 points)\n",
    "# Hints: use \"terms\"\n",
    "\n",
    "terms[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84947897",
   "metadata": {
    "id": "q-CjFLXH3BJg"
   },
   "source": [
    "Your answer shoule be like:\n",
    "\"[{'id': 'APW_ENG_20100101.0001', 'field': 'body', 'term': 'about', 'count': 1},\n",
    " {'id': 'APW_ENG_20100101.0001', 'field': 'body', 'term': 'abuse', 'count': 1},\n",
    " ...}]\"\n",
    "\n",
    "Each record has four fields:\n",
    "* `id`, with the identifier for the document;\n",
    "* `field`, with the region of the document containing a given term;\n",
    "* `term`, with the lower-cased term; and\n",
    "* `count`, with the number of times each term occurred in that field and document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f37777",
   "metadata": {
    "id": "2H5yBvEVNUPr"
   },
   "source": [
    "## Computing Term Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537326e",
   "metadata": {
    "id": "qhDt23kKv0Uy"
   },
   "source": [
    "If we look at the most frequent terms for a given document, we mostly see common function words, such as `the`, `and`, and `of`. Start exploring the dataset by computing some of these basic term statistics. You can make your life easier using data frame libraries such as `pandas`, core python libraries such as `collections`, or just simple list comprehensions.\n",
    "\n",
    "Feel free to define helper functions in your code before computing the statistics we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9080e8",
   "metadata": {
    "id": "2Zy5qR562nZ5"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the 10 terms from document APW_ENG_20100101.0001 with the highest count. (5 points)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filtered_dicts = [d for d in terms if d['id'] == 'APW_ENG_20100101.0001']\n",
    "df = pd.DataFrame(filtered_dicts)\n",
    "top_10_terms = df.sort_values(by='count', ascending=False).head(10)\n",
    "top_10_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20d173",
   "metadata": {
    "id": "U7OwCo0w5R1q"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the 10 terms with the highest total count in the corpus. (5 points)\n",
    "\n",
    "frequency = []\n",
    "terms_set = set()\n",
    "distinct_terms_to_frequency_dict = {}\n",
    "\n",
    "for d in terms:\n",
    "\n",
    "    if d['term'] in terms_set:\n",
    "\n",
    "        distinct_terms_to_frequency_dict[d['term']] += d['count']\n",
    "\n",
    "    else:\n",
    "\n",
    "        distinct_terms_to_frequency_dict[d['term']] = d['count']\n",
    "        terms_set.add(d['term'])\n",
    "\n",
    "frequency = [(term, count) for term, count in distinct_terms_to_frequency_dict.items()]\n",
    "sorted_frequency = sorted(frequency, key = lambda x : x[1], reverse=True)\n",
    "print(sorted_frequency[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b08f4a",
   "metadata": {
    "id": "dnNEUACW23Dd"
   },
   "source": [
    "Raw counts may not be the most informative statistic. One common improvement is to use *inverse document frequency*, the inverse of the proportion of documents that contain a given term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228c3ae",
   "metadata": {
    "id": "uiUA502P2QkH"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the number of distinct documents in the collection. (5 points)\n",
    "N = 0\n",
    "\n",
    "def number_of_distinct_documents():\n",
    "\n",
    "    distinct_doc_ids = set()\n",
    "\n",
    "    for dict in terms:\n",
    "        distinct_doc_ids.add(dict['id'])\n",
    "    \n",
    "    return len(distinct_doc_ids)\n",
    "\n",
    "N = number_of_distinct_documents()\n",
    "\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f20684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the number of distinct documents each term appears in \n",
    "# and store in a dictionary. (5 points)\n",
    "\n",
    "def compute_term_document_counts():\n",
    "\n",
    "    term_to_distinct_documents_dict = {}\n",
    "\n",
    "    for d in terms:\n",
    "\n",
    "        term = d['term']\n",
    "        doc_id = d['id']\n",
    "\n",
    "        if term in term_to_distinct_documents_dict:\n",
    "\n",
    "            if doc_id not in term_to_distinct_documents_dict[term]:\n",
    "\n",
    "                term_to_distinct_documents_dict[term].add(doc_id)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            term_to_distinct_documents_dict[term] = {doc_id}\n",
    "        \n",
    "    return term_to_distinct_documents_dict\n",
    "        \n",
    "\n",
    "df = compute_term_document_counts()\n",
    "\n",
    "for key, value in df.items():\n",
    "    df[key] = len(value)\n",
    "      \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1d1a6",
   "metadata": {
    "id": "_XMPAKYNCq6Y"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the relative document frequency of 'the', (5 points)\n",
    "# i.e., the number of documents that contain 'the' divided by N.\n",
    "\n",
    "print(df['the'] / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c83886",
   "metadata": {
    "id": "ohFmwtc7Chy3"
   },
   "source": [
    "Empricially, we usually see better retrieval results if we rescale term frequency (within documents) and inverse document frequency (across documents) with the log function. Let the `tfidf` of term _t_ in document _d_ be:\n",
    "```\n",
    "tfidf(t, d) = log(count(t, d) + 1) * log(N / df(t))\n",
    "```\n",
    "\n",
    "Later in the course, we will show a probabilistic derivation of this quantity based on smoothing language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64194b66",
   "metadata": {
    "id": "Fmyj4v_uHdyo"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the tf-idf value for each term in each document. (10 points)\n",
    "# Take the raw term data and add a tfidf field to each record.\n",
    "\n",
    "import math\n",
    "\n",
    "tfidf_terms = None\n",
    "\n",
    "for d in terms:\n",
    "    tf= math.log(d['count'] + 1, 10)\n",
    "    idf = math.log(N / df[d['term']], 10)\n",
    "    tfidf = tf*idf\n",
    "\n",
    "    d['tfidf'] = math.log(d['count'] + 1, 10) * math.log(N / df[d['term']], 10)\n",
    "\n",
    "tfidf_terms = terms\n",
    "\n",
    "print(tfidf_terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514f3e3",
   "metadata": {
    "id": "NlXQmMO9HxH0"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the 20 term-document pairs with the highest tf-idf values. (5 points)\n",
    "\n",
    "df = pd.DataFrame(tfidf_terms)\n",
    "\n",
    "top_20_term_document_pairs_list = df.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "top_20_term_document_pairs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977259b",
   "metadata": {
    "id": "f61xitl1IApl"
   },
   "source": [
    "## Plotting Term Distributions\n",
    "\n",
    "Besides frequencies and tf-idf values within documents, it is often helpful to look at the distrubitions of word frequencies in the whole collection. In class, we talk about the Zipf distribution of word rank versus frequency and Heaps' Law relating the number of distinct words to the number of tokens.\n",
    "\n",
    "We might examine these distributions to see, for instance, if an unexpectedly large number of very rare terms occurs, which might indicate noise added to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c9c66",
   "metadata": {
    "id": "fsM5k1_5Jj7Y"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute a list of the distinct words in this collection and sort it in descending order of frequency.\n",
    "# Thus frequency[0] should contain the word \"the\" and the count 62216. (5 points)\n",
    "\n",
    "frequency = []\n",
    "\n",
    "terms_set = set()\n",
    "\n",
    "distinct_terms_to_frequency_dict = {}\n",
    "\n",
    "for d in terms:\n",
    "\n",
    "    if d['term'] in terms_set:\n",
    "\n",
    "        distinct_terms_to_frequency_dict[d['term']] += d['count']\n",
    "\n",
    "    else:\n",
    "\n",
    "        distinct_terms_to_frequency_dict[d['term']] = d['count']\n",
    "        terms_set.add(d['term'])\n",
    "\n",
    "\n",
    "frequency = [(term, count) for term, count in distinct_terms_to_frequency_dict.items()]\n",
    "\n",
    "sorted_frequency = sorted(frequency, key = lambda x : x[1], reverse=True)\n",
    "\n",
    "print(sorted_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a0c15",
   "metadata": {
    "id": "hdtc14EULkxS"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot a graph of the log of the rank (starting at 1) on the x-axis,\n",
    "# against the log of the frequency on the y-axis. You may use the matplotlib\n",
    "# or other library. (5 points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_val = list(range(1, len(sorted_frequency) + 1))\n",
    "\n",
    "y_val = [element[1] for element in sorted_frequency]\n",
    "\n",
    "log_rank = np.log(x_val)\n",
    "log_frequency = np.log(y_val)\n",
    "\n",
    "plt.plot(log_rank, log_frequency )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd6207",
   "metadata": {
    "id": "-WdHjFCSC7WC"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the number of tokens in the corpus. (5 points)\n",
    "# Remember to count each occurrence of each word.\n",
    "\n",
    "ntokens = 0\n",
    "\n",
    "for element in sorted_frequency:\n",
    "\n",
    "    ntokens += element[1]\n",
    "\n",
    "print(ntokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4ad43",
   "metadata": {
    "id": "V_7wOcqKAz9m"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the proportion of tokens made up by the top 10 most\n",
    "# frequent words. (5 points)\n",
    "\n",
    "top_10_most_frequent_words_total = 0\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    top_10_most_frequent_words_total += sorted_frequency[i][1]\n",
    "\n",
    "print(top_10_most_frequent_words_total / ntokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab6c23",
   "metadata": {
    "id": "uF-1VxcZBXMM"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the proportion of tokens made up by the words that occur\n",
    "# exactly once in this collection. (5 points)\n",
    "\n",
    "words_that_occur_exactly_once = 0\n",
    "\n",
    "for element in sorted_frequency:\n",
    "\n",
    "    if element[1] == 1:\n",
    "\n",
    "        words_that_occur_exactly_once += 1\n",
    "\n",
    "print(words_that_occur_exactly_once / ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b80ad6",
   "metadata": {},
   "source": [
    "\n",
    "For this assignment so far, you've worked with data that's already been extracted, tokenized, and counted. In this final section, you'll briefly explore acquiring new data.\n",
    "\n",
    "Find a collection of documents that you're interested in. For the statistics to be meaningful, this collection should have at least 1,000 words.\n",
    "\n",
    "The format could be anything you can extract text from: HTML, PDF, MS PowerPoint, chat logs, etc.\n",
    "\n",
    "The collection should be in a natural language, not mostly code or numerical data. It could be in English or in any other language.\n",
    "\n",
    "The final project for this course will involve designing an information retrieval task on some dataset. You could use this exercise to think about what kind of data you might be interested in, although that is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41502c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to download and extract the text from the collection.  (5 points)\n",
    "import kaggle\n",
    "import os\n",
    "\n",
    "!wsl wget \"https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee17345",
   "metadata": {},
   "source": [
    "**TODO**: Describe choices you make about what contents to keep. (2 points)\n",
    "\n",
    "I want to keep every word. I want to remove all punctuation such as commas, and periods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318fad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Data acquisition code here. (5 points)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "directory_path = r\"C:\\Users\\vijay\\Desktop\\6200 Projects\\Assignment 1\\sport\"\n",
    "\n",
    "file_paths = glob.glob(os.path.join(directory_path, '*'))\n",
    "\n",
    "list_of_dicts = []\n",
    "term_to_frequency_in_document_dict = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                element = dict()\n",
    "                element['id'] = os.path.basename(file_path)\n",
    "                element['term'] = token\n",
    "                \n",
    "                term_to_frequency_in_document_dict[token] = term_to_frequency_in_document_dict.get(token, 0) + 1\n",
    "                list_of_dicts.append(element)\n",
    "\n",
    "for element in list_of_dicts:\n",
    "    element['count'] = term_to_frequency_in_document_dict[element['term']]\n",
    "\n",
    "print(list_of_dicts[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d45fb",
   "metadata": {},
   "source": [
    "**TODO**: Write code to tokenize the text and count the resulting terms in each document. Describe your tokenization approach here.\n",
    "\n",
    "Each term may also be associated with a field, such as `body` and `title` in the newswire collection above. Describe the different fields in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce022ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenization code here. (5 points)\n",
    "\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "cleaned_tokens = []\n",
    "\n",
    "for d in list_of_dicts:\n",
    "\n",
    "    cleaned_token = d['term'].translate(translator)\n",
    "\n",
    "    if cleaned_token:\n",
    "\n",
    "        cleaned_tokens.append(cleaned_token)\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57edf",
   "metadata": {},
   "source": [
    "**TODO**: Plot a graph of the log rank against log frequency for your collection, as you did for the sample collection above. What do you observe about the differences between the distributions in these two collections? (2 points)\n",
    "\n",
    "\n",
    "x_val = list(range(1, len(sorted_frequency) + 1))\n",
    "\n",
    "y_val = [element[1] for element in sorted_frequency]\n",
    "\n",
    "log_rank = np.log(x_val)\n",
    "log_frequency = np.log(y_val)\n",
    "\n",
    "\n",
    "plt.plot(log_rank, log_frequency )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced9774",
   "metadata": {},
   "source": [
    "# Inverted Index\n",
    "Create an inverted index of the data in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67751836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Do : Create an inverted index of the corpus extracted in the 1st part. (10 points)\n",
    "\n",
    "def create_inverted_index ():\n",
    "\n",
    "  inverted_index = {}\n",
    "\n",
    "  for element in list_of_dicts:\n",
    "\n",
    "    if element['term'] in inverted_index :\n",
    "\n",
    "      if element['id'] in inverted_index[element['term']] :\n",
    "\n",
    "        inverted_index[element['term']][element['id']] += 1\n",
    "      \n",
    "      else :\n",
    "\n",
    "        inverted_index[element['term']][element['id']] = 1\n",
    "    \n",
    "    else:\n",
    "      inverted_index[element['term']] = {} \n",
    "\n",
    "\n",
    "  return inverted_index\n",
    "\n",
    "\n",
    "inverted_index = create_inverted_index()\n",
    "\n",
    "print(inverted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14581630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write unit tests to validate your inverted index. (5 points)\n",
    "\n",
    "#t is the term\n",
    "#ev is the expected value\n",
    "\n",
    "def validate_index(t, ev):\n",
    "\n",
    "    result = inverted_index[t] \n",
    "    expected_value = ev\n",
    "\n",
    "    if result == expected_value:\n",
    "\n",
    "        print(\"Good Job, test passed!\")\n",
    "    \n",
    "    else:\n",
    "\n",
    "        print(\"Sorry, test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_index('Claxton', {'sport_1.txt': 5, 'sport_58.txt': 1, 'sport_60.txt': 1, 'sport_68.txt': 1})\n",
    "validate_index('first', {'sport_1.txt': 2, 'sport_14.txt': 1, 'sport_2.txt': 1, 'sport_22.txt': 1, 'sport_25.txt': 1, 'sport_26.txt': 1, 'sport_34.txt': 1, 'sport_35.txt': 1, 'sport_40.txt': 1, 'sport_42.txt': 1, 'sport_44.txt': 2, 'sport_52.txt': 1, 'sport_53.txt': 1, 'sport_57.txt': 2, 'sport_58.txt': 3, 'sport_60.txt': 1, 'sport_61.txt': 4, 'sport_66.txt': 5, 'sport_68.txt': 2, 'sport_69.txt': 1, 'sport_71.txt': 1, 'sport_75.txt': 3, 'sport_76.txt': 2, 'sport_83.txt': 1, 'sport_84.txt': 1, 'sport_85.txt': 1, 'sport_86.txt': 1, 'sport_89.txt': 1, 'sport_9.txt': 1, 'sport_93.txt': 1, 'sport_95.txt': 1, 'sport_96.txt': 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
